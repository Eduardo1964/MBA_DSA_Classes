min_data <- apply(data, 2, min)
scaled <- scale(data,center = min_data, scale = max_data - min_data)
nn <- neuralnet(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat
,data = train_data,hidden = c(5,4,3,2),linear.output=T)
plot(nn)
pr.nn <- compute(nn,test_data[,1:13])
pr.nn_ <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
test.r <- (test_data$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test)
set.seed(0)
max_data <- apply(data, 2, max)
min_data <- apply(data, 2, min)
scaled <- scale(data,center = min_data, scale = max_data - min_data)
nn <- neuralnet(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat
,data = train_data,hidden = c(5,4,3,2),linear.output=T)
plot(nn)
pr.nn <- compute(nn,test_data[,1:13])
pr.nn_ <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
test.r <- (test_data$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test)
set.seed(0)
max_data <- apply(data, 2, max)
min_data <- apply(data, 2, min)
scaled <- scale(data,center = min_data, scale = max_data - min_data)
nn <- neuralnet(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat
,data = train_data,hidden = 5,linear.output=T)
#plot(nn)
pr.nn <- compute(nn,test_data[,1:13])
pr.nn_ <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
test.r <- (test_data$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test)
set.seed(0)
max_data <- apply(data, 2, max)
min_data <- apply(data, 2, min)
scaled <- scale(data,center = min_data, scale = max_data - min_data)
nn <- neuralnet(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat
,data = train_data,hidden = 10,linear.output=T)
#plot(nn)
pr.nn <- compute(nn,test_data[,1:13])
pr.nn_ <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
test.r <- (test_data$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test)
set.seed(0)
max_data <- apply(data, 2, max)
min_data <- apply(data, 2, min)
scaled <- scale(data,center = min_data, scale = max_data - min_data)
nn <- neuralnet(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat
,data = train_data,hidden = 40,linear.output=T)
#plot(nn)
pr.nn <- compute(nn,test_data[,1:13])
pr.nn_ <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
test.r <- (test_data$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test)
set.seed(0)
max_data <- apply(data, 2, max)
min_data <- apply(data, 2, min)
scaled <- scale(data,center = min_data, scale = max_data - min_data)
nn <- neuralnet(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat
,data = train_data,hidden = c(5,4,3,2),linear.output=T)
#plot(nn)
pr.nn <- compute(nn,test_data[,1:13])
pr.nn_ <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
test.r <- (test_data$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test)
set.seed(0)
max_data <- apply(data, 2, max)
min_data <- apply(data, 2, min)
scaled <- scale(data,center = min_data, scale = max_data - min_data)
nn <- neuralnet(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat
,data = train_data,hidden = c(5,2),linear.output=T)
#plot(nn)
pr.nn <- compute(nn,test_data[,1:13])
pr.nn_ <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
test.r <- (test_data$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test)
set.seed(0)
max_data <- apply(data, 2, max)
min_data <- apply(data, 2, min)
scaled <- scale(data,center = min_data, scale = max_data - min_data)
nn <- neuralnet(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat
,data = train_data,hidden = c(5,4),linear.output=T)
#plot(nn)
pr.nn <- compute(nn,test_data[,1:13])
pr.nn_ <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
test.r <- (test_data$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test)
set.seed(0)
max_data <- apply(data, 2, max)
min_data <- apply(data, 2, min)
scaled <- scale(data,center = min_data, scale = max_data - min_data)
nn <- neuralnet(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat
,data = train_data,hidden = c(5,4,2),linear.output=T)
#plot(nn)
pr.nn <- compute(nn,test_data[,1:13])
pr.nn_ <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
test.r <- (test_data$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test)
set.seed(0)
max_data <- apply(data, 2, max)
min_data <- apply(data, 2, min)
scaled <- scale(data,center = min_data, scale = max_data - min_data)
nn <- neuralnet(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat
,data = train_data,hidden = 20,linear.output=T)
#plot(nn)
pr.nn <- compute(nn,test_data[,1:13])
pr.nn_ <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
test.r <- (test_data$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test)
library(ISLR)
library(neuralnet)
#Olhar os dados - data wrangling?
data <- College
View(data)
#private = as.numeric(College$Private)-1
private <- ifelse(data$Private == 'Yes', 1, 0)
private
View(data)
data <- data[,2:18]
max_data <- apply(data, 2, max)
min_data <- apply(data, 2, min)
scaled <- data.frame(scale(data,center = min_data, scale = max_data - min_data))
View(scaled)
scaled$Private <- private
View(scaled)
index = sample(1:nrow(data),round(0.70*nrow(data)))
train_data <- as.data.frame(scaled[index,])
test_data <- as.data.frame(scaled[-index,])
n = names(train_data)
f <- as.formula(paste("Private ~", paste(n[!n %in% "Private"], collapse = " + ")))
f
#is.na(data)
#View(data)
set.seed(0)
#private = as.numeric(College$Private)-1
private <- ifelse(data$Private == 'Yes', 1, 0)
data <- data[,2:18]
max_data <- apply(data, 2, max)
min_data <- apply(data, 2, min)
scaled <- data.frame(scale(data,center = min_data, scale = max_data - min_data))
scaled$Private <- private
index = sample(1:nrow(data),round(0.70*nrow(data)))
train_data <- as.data.frame(scaled[index,])
test_data <- as.data.frame(scaled[-index,])
n = names(train_data)
f <- as.formula(paste("Private ~", paste(n[!n %in% "Private"], collapse = " + ")))
?neuralnet
nn <- neuralnet(formula = f, data = train_data, hidden = c(20,10,5,3), linear.output = FALSE,
act.fct = 'logistic')
View(train_data)
scaled$Private <- private
index = sample(1:nrow(data),round(0.70*nrow(data)))
train_data <- as.data.frame(scaled[index,])
test_data <- as.data.frame(scaled[-index,])
scaled$Private <- private
data <- College
#is.na(data)
#View(data)
set.seed(0)
#private = as.numeric(College$Private)-1
private <- ifelse(data$Private == 'Yes', 1, 0)
data <- data[,2:18]
max_data <- apply(data, 2, max)
min_data <- apply(data, 2, min)
scaled <- data.frame(scale(data,center = min_data, scale = max_data - min_data))
#private = as.numeric(College$Private)-1
private <- ifelse(data$Private == 'Yes', 1, 0)
data <- data[,2:18]
max_data <- apply(data, 2, max)
min_data <- apply(data, 2, min)
scaled <- data.frame(scale(data,center = min_data, scale = max_data - min_data))
scaled$Private <- private
#private = as.numeric(College$Private)-1
private <- ifelse(data$Private == 'Yes', 1, 0)
private
data <- College
#private = as.numeric(College$Private)-1
private <- ifelse(data$Private == 'Yes', 1, 0)
data <- data[,2:18]
max_data <- apply(data, 2, max)
min_data <- apply(data, 2, min)
scaled <- data.frame(scale(data,center = min_data, scale = max_data - min_data))
scaled$Private <- private
index = sample(1:nrow(data),round(0.70*nrow(data)))
train_data <- as.data.frame(scaled[index,])
test_data <- as.data.frame(scaled[-index,])
n = names(train_data)
f <- as.formula(paste("Private ~", paste(n[!n %in% "Private"], collapse = " + ")))
nn <- neuralnet(formula = f, data = train_data, hidden = c(20,10,5,3), linear.output = FALSE,
act.fct = 'logistic')
pr.nn <- compute(nn,test_data[,1:17])
pr.nn$net.result
#Explica sapply
pr.nn$net.result <- sapply(pr.nn$net.result,round,digits=0)
pr.nn$net.result
table(test_data$Private,pr.nn$net.result)
Acc <- (59+155) / (59+155+10+9)
set.seed(0)
library(caret)
model <- glm(f, family="binomial", data=train_data)
test_data$predicted <- round(predict(model, test_data, type="response"), digits = 0)
table(test_data$Private, test_data$predicted)
Acc_logit <- (58+153) / (58+153+11+11)
pacotes <- c("MASS","neuralnet","ISLR","mlbench","neuralnet","rpart")
if(sum(as.numeric(!pacotes %in% installed.packages())) != 0){
instalador <- pacotes[!pacotes %in% installed.packages()]
for(i in 1:length(instalador)) {
install.packages(instalador, dependencies = T)
break()}
sapply(pacotes, require, character = T)
} else {
sapply(pacotes, require, character = T)
}
#Conceito de funÃ§Ã£o
# y = f(x)
# y = a +b1*x1 +b2*x2...
#install.packages("MASS")
library(MASS)
#Baixa os dados
data <- Boston
#Uma olhada nos dados
head(data)
#Temos valores nulos?
data[is.na(data) == TRUE]
#Train-Test Split
train_test_split_index <- 0.8 * nrow(data)
train <- data.frame(data[1:train_test_split_index,])
test <- data.frame(data[(train_test_split_index+1): nrow(data),])
#CART
library(rpart)
# grow tree
fit <- rpart(Kyphosis ~ Age + Number + Start,
method="class", data=kyphosis)
data <- kyphosis
View(data)
# Ã¡rvore
fit <- rpart(medv ~.,method="class", data=train)
rpart.plot(fit_tree, type = 1)
pacotes <- c("MASS","neuralnet","ISLR","mlbench","neuralnet","rpart","rpart.plot")
if(sum(as.numeric(!pacotes %in% installed.packages())) != 0){
instalador <- pacotes[!pacotes %in% installed.packages()]
for(i in 1:length(instalador)) {
install.packages(instalador, dependencies = T)
break()}
sapply(pacotes, require, character = T)
} else {
sapply(pacotes, require, character = T)
}
rpart.plot(fit_tree, type = 1)
library("rpart.plot")
library("MASS")
library("rpart")
library("rpart.plot")
set.seed(0)
#Baixa os dados
data <- Boston
#Temos valores nulos?
data[is.na(data) == TRUE]
#Train-Test Split
train_test_split_index <- 0.8 * nrow(data)
train <- data.frame(data[1:train_test_split_index,])
test <- data.frame(data[(train_test_split_index+1): nrow(data),])
#CART
# Ã¡rvore
fit_tree <- rpart(medv ~.,method="class", data=train)
rpart.plot(fit_tree, type = 1)
tree_predict <- predict(fit_tree,test)
mse_tree <- mean((tree_predict - test$medv)^2)
#Padronizar dados para melhor performance
#Explicar apply
set.seed(0)
max_data <- apply(data, 2, max)
min_data <- apply(data, 2, min)
scaled <- scale(data,center = min_data, scale = max_data - min_data)
index = sample(1:nrow(data),round(0.70*nrow(data)))
train_data <- as.data.frame(scaled[index,])
test_data <- as.data.frame(scaled[-index,])
#install.packages("neuralnet")
library(neuralnet)
#abrir o CRAN para mostrar
#Fit de neuralnet
#Executar testes com diferentes arquiteturas
nn <- neuralnet(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat,data=train_data,hidden=c(5,4,3,2),linear.output=T)
plot(nn)
pr.nn <- compute(nn,test_data[,1:13])
pr.nn_ <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
test.r <- (test_data$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
MSE_nn <- sum((test.r - pr.nn_)^2)/nrow(test)
mean((pr.nn_ - test.r)^2)
mse_tree <- sum((fit_tree - tree_predict)^2)/nrow(test)
fit_tree
tree_predict
fit_tree$y
tree_predict
mse_tree <- sum((test - tree_predict)^2)/nrow(test)
mse_tree <- mean((tree_predict - test$medv)^2)
mse_tree2 <- sum((test$medv - tree_predict)^2)/nrow(test)
mean((pr.nn_ - test.r)^2)
#Conceito de funÃ§Ã£o
# y = f(x)
# y = a +b1*x1 +b2*x2...
#install.packages("MASS")
library("MASS")
library("rpart")
set.seed(0)
#Baixa os dados
data <- Boston
#Uma olhada nos dados
head(data)
#Temos valores nulos?
data[is.na(data) == TRUE]
#Train-Test Split
train_test_split_index <- 0.8 * nrow(data)
train <- data.frame(data[1:train_test_split_index,])
test <- data.frame(data[(train_test_split_index+1): nrow(data),])
#CART
# Ã¡rvore
fit_tree <- rpart(medv ~.,method="class", data=train)
tree_predict <- predict(fit_tree,test)
mse_tree <- mean((tree_predict - test$medv)^2)
#Padronizar dados para melhor performance
#Explicar apply
set.seed(0)
max_data <- apply(data, 2, max)
min_data <- apply(data, 2, min)
scaled <- scale(data,center = min_data, scale = max_data - min_data)
index = sample(1:nrow(data),round(0.70*nrow(data)))
train_data <- as.data.frame(scaled[index,])
test_data <- as.data.frame(scaled[-index,])
library(neuralnet)
#abrir o CRAN para mostrar
#Fit de neuralnet
#Executar testes com diferentes arquiteturas
nn <- neuralnet(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat,data=train_data,hidden=c(5,4,3,2),linear.output=T)
plot(nn)
pr.nn <- compute(nn,test_data[,1:13])
pr.nn_ <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
test.r <- (test_data$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
MSE_nn <- mean((pr.nn_ - test.r)^2)
plot(test_data$medv,type = 'l',col="red",xlab = "x", ylab = "Valor Residencia")
lines(pr.nn$net.result,col = "blue")
#Conceito de funÃ§Ã£o
# y = f(x)
# y = a +b1*x1 +b2*x2...
#install.packages("MASS")
library("MASS")
library("rpart")
set.seed(0)
#Baixa os dados
data <- Boston
#Uma olhada nos dados
head(data)
#Temos valores nulos?
data[is.na(data) == TRUE]
#Train-Test Split
train_test_split_index <- 0.8 * nrow(data)
train <- data.frame(data[1:train_test_split_index,])
test <- data.frame(data[(train_test_split_index+1): nrow(data),])
#CART
# Ã¡rvore
fit_tree <- rpart(medv ~.,method="anova", data=train)
tree_predict <- predict(fit_tree,test)
mse_tree <- mean((tree_predict - test$medv)^2)
#Padronizar dados para melhor performance
#Explicar apply
set.seed(0)
max_data <- apply(data, 2, max)
min_data <- apply(data, 2, min)
scaled <- scale(data,center = min_data, scale = max_data - min_data)
index = sample(1:nrow(data),round(0.70*nrow(data)))
train_data <- as.data.frame(scaled[index,])
test_data <- as.data.frame(scaled[-index,])
library(neuralnet)
#abrir o CRAN para mostrar
#Fit de neuralnet
#Executar testes com diferentes arquiteturas
nn <- neuralnet(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat,data=train_data,hidden=c(5,4,3,2),linear.output=T)
plot(nn)
pr.nn <- compute(nn,test_data[,1:13])
pr.nn_ <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
test.r <- (test_data$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
MSE_nn <- mean((pr.nn_ - test.r)^2)
library(ISLR)
library(neuralnet)
#Olhar os dados - data wrangling?
data <- College
#is.na(data)
#View(data)
#private = as.numeric(College$Private)-1
private <- ifelse(data$Private == 'Yes', 1, 0)
#Padronizar dados para melhor performance
data <- data[,2:18]
max_data <- apply(data, 2, max)
min_data <- apply(data, 2, min)
scaled <- data.frame(scale(data,center = min_data, scale = max_data - min_data))
#Inclui variÃ¡vel explicada (target)
scaled$Private <- private
set.seed(0)
#train test split
index = sample(1:nrow(data),round(0.70*nrow(data)))
train_data <- as.data.frame(scaled[index,])
test_data <- as.data.frame(scaled[-index,])
#Utiliza o neuralnet
set.seed(0)
n = names(train_data)
f <- as.formula(paste("Private ~", paste(n[!n %in% "Private"], collapse = " + ")))
nn <- neuralnet(f,data=train_data,hidden=c(20,10,5,3),linear.output=F)
plot(nn)
pr.nn <- compute(nn,test_data[,1:17])
#Explica sapply
pr.nn$net.result <- sapply(pr.nn$net.result,round,digits=0)
pr.nn$net.result
table(test_data$Private,pr.nn$net.result)
Acc <- (62+155) / (62+155+7+9)
# Ã¡rvore
fit_tree <- rpart(f,method="class", data=train_data)
tree_predict <- predict(fit_tree,test)
tree_predict <- predict(fit_tree,test_data)
table(test_data$Private,tree_predict)
View(tree_predict)
tree_predict <- predict(fit_tree,test_data,type = "class")
table(test_data$Private,tree_predict)
Acc_tree <- (58+159) / (58+159+11+5)
nn <- neuralnet(f,data=train_data,hidden=c(5,4),linear.output=F)
pr.nn <- compute(nn,test_data[,1:17])
#Explica sapply
pr.nn$net.result <- sapply(pr.nn$net.result,round,digits=0)
pr.nn$net.result
table(test_data$Private,pr.nn$net.result)
Acc <- (62+158) / (62+158+7+6)
set.seed(0)
#Conceito de funÃ§Ã£o
# y = f(x)
# y = a +b1*x1 +b2*x2...
#install.packages("MASS")
library("MASS")
library("rpart")
set.seed(0)
#Baixa os dados
data <- Boston
#Uma olhada nos dados
head(data)
#Temos valores nulos?
data[is.na(data) == TRUE]
#Train-Test Split
train_test_split_index <- 0.8 * nrow(data)
train <- data.frame(data[1:train_test_split_index,])
test <- data.frame(data[(train_test_split_index+1): nrow(data),])
#CART
# Ã¡rvore
fit_tree <- rpart(medv ~.,method="anova", data=train)
tree_predict <- predict(fit_tree,test)
mse_tree <- mean((tree_predict - test$medv)^2)
#Padronizar dados para melhor performance
#Explicar apply
set.seed(0)
max_data <- apply(data, 2, max)
min_data <- apply(data, 2, min)
scaled <- scale(data,center = min_data, scale = max_data - min_data)
index = sample(1:nrow(data),round(0.70*nrow(data)))
train_data <- as.data.frame(scaled[index,])
test_data <- as.data.frame(scaled[-index,])
library(neuralnet)
#abrir o CRAN para mostrar
#Fit de neuralnet
#Executar testes com diferentes arquiteturas
nn <- neuralnet(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat,data=train_data,hidden=c(5,4,3,2),linear.output=T)
plot(nn)
pr.nn <- compute(nn,test_data[,1:13])
pr.nn_ <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
test.r <- (test_data$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
MSE_nn <- mean((pr.nn_ - test.r)^2)
plot(test_data$medv,type = 'l',col="red",xlab = "x", ylab = "Valor Residencia")
lines(pr.nn$net.result,col = "blue")
library(ISLR)
library(neuralnet)
#Olhar os dados - data wrangling?
data <- College
#is.na(data)
#View(data)
#private = as.numeric(College$Private)-1
private <- ifelse(data$Private == 'Yes', 1, 0)
#Padronizar dados para melhor performance
data <- data[,2:18]
max_data <- apply(data, 2, max)
min_data <- apply(data, 2, min)
scaled <- data.frame(scale(data,center = min_data, scale = max_data - min_data))
#Inclui variÃ¡vel explicada (target)
scaled$Private <- private
set.seed(0)
#train test split
index = sample(1:nrow(data),round(0.70*nrow(data)))
train_data <- as.data.frame(scaled[index,])
test_data <- as.data.frame(scaled[-index,])
#Utiliza o neuralnet
set.seed(0)
n = names(train_data)
f <- as.formula(paste("Private ~", paste(n[!n %in% "Private"], collapse = " + ")))
nn <- neuralnet(f,data=train_data,hidden=c(5,4),linear.output=F)
plot(nn)
pr.nn <- compute(nn,test_data[,1:17])
#Explica sapply
pr.nn$net.result <- sapply(pr.nn$net.result,round,digits=0)
pr.nn$net.result
table(test_data$Private,pr.nn$net.result)
Acc <- (62+158) / (62+158+7+6)
#CART comparaÃ§Ã£o
set.seed(0)
# Ã¡rvore
fit_tree <- rpart(f,method="class", data=train_data)
tree_predict <- predict(fit_tree,test_data,type = "class")
table(test_data$Private,tree_predict)
Acc_tree <- (58+159) / (58+159+11+5)
