stop("Number of layers is unequal to the number of weight matrices in the model")
}
}
layers = 2
model = modelRBM
if (nrow(testX) != length(testY)) {
stop("testY and testX data should be of equal size")
}
if (any(!is.numeric(testX))) {
stop('Sorry the data has non-numeric values, the function is executed')
}
if (any(!is.finite(testX))) {
stop('Sorry this function cannot handle NAs or non-finite data')
}
if (layers == 2) {
if (length(model) > 2) {
stop("Number of layers is unequal to the number of weight matrices in the model")
}
}
result.dat <- data.frame('y' = testXY, 'y.pred'= rep(0,length(testXY)))
layers = 2
model = modelRBM
if (nrow(testX) != length(testY)) {
stop("testY and testX data should be of equal size")
}
if (any(!is.numeric(testX))) {
stop('Sorry the data has non-numeric values, the function is executed')
}
if (any(!is.finite(testX))) {
stop('Sorry this function cannot handle NAs or non-finite data')
}
if (layers == 2) {
if (length(model) > 2) {
stop("Number of layers is unequal to the number of weight matrices in the model")
}
}
result.dat <- data.frame('y' = testY, 'y.pred'= rep(0,length(testY)))
# Creating binarized matrix of all the possible testXY and add bias term
y <- cbind(1, LabelBinarizer(unique(testXY)))
# Name the rows after the possible testXY:
rownames(y) <- unique(testXY)
# Name the rows after the possible testXY:
rownames(y) <- unique(testY)
p <- PredictRBM(test = testX, labels = TestY, model = modelRBM)
#roda modelo
modelRBM <- RBM(x = train$Amount, n.iter = 1000, n.hidden = 20)
ReconstructRBM(test = test$Amount[100, ], model = modelRBM)
test$Amount[100, ]
View(test)
View(test)
ReconstructRBM(test = test[100, ], model = modelRBM)
ReconstructRBM(test = test, model = modelRBM)
data(movie_reviews)
data(george_reviews)
modelRBM[["trained.weights"]]
modelRBM[["trained.y.weights"]]
data(Fashion)
image(matrix(Fashion$trainX[100, ], nrow = 28))
image(matrix(Fashion$trainX[,100], nrow = 28))
MNIST$trainY[100]
Fashion$trainY[100]
train <- Fashion$trainX
modelRBM <- RBM(x = train, n.iter = 1000, n.hidden = 100, size.minibatch = 10, plot = TRUE)
test <- Fashion$trainX[,100]
train <- t(Fashion$trainX)
modelRBM <- RBM(x = train, n.iter = 1000, n.hidden = 100, size.minibatch = 10, plot = TRUE)
test <- MNIST$testX
ReconstructRBM(test = t(test[100, ]), model = modelRBM)
ReconstructRBM(test = test[100], model = modelRBM)
View(test)
test <- Fashion$testX
ReconstructRBM(test = test[100], model = modelRBM)
test <- Fashion$testX
test[10]
test <- t(Fashion$testX)
ReconstructRBM(test = test[100], model = modelRBM)
test[100]
train[10]
View(train)
Fashion$trainY
Fashion$trainX
View(modelRBM)
modelRBM[["trained.weights"]]
test[100, ]
ReconstructRBM(test = test[100,], model = modelRBM)
library(dplyr)
library(RBM)
data(Fashion)
image(matrix(Fashion$trainX[,100], nrow = 28))
image(matrix(Fashion$trainX[,102], nrow = 28))
Fashion$trainY[102]
train <- t(Fashion$trainX)
modelRBM <- RBM(x = train, n.iter = 1000, n.hidden = 100, size.minibatch = 10, plot = TRUE)
test <- t(Fashion$testX)
ReconstructRBM(test = test[100,], model = modelRBM)
ReconstructRBM(test = test[102,], model = modelRBM)
#DBN
modelDBN <- DBN(x = train, n.iter = 1000, n.hidden = 100, size.minibatch = 10, plot = TRUE)
#DBN
modelDBN <- DBN(x = train, nodes = 100, n.iter = 1000, size.minibatch = 10)
#DBN
modelDBN <- StackRBM(x = train, nodes = 100, n.iter = 1000, size.minibatch = 10)
#DBN
modelDBN <- StackRBM(x = train, layers = 100, n.iter = 1000, size.minibatch = 10)
#RBM com classificação
train <- Fashion$trainX
TrainY <- Fashion$trainY
modelClassRBM <- RBM(x = train, y = TrainY, n.iter = 1000, n.hidden = 100, size.minibatch = 10)
#RBM com classificação
train <- t(Fashion$trainX)
#RBM com classificação
train <- t(Fashion$trainX)
TrainY <- t(Fashion$trainY)
modelClassRBM <- RBM(x = train, y = TrainY, n.iter = 1000, n.hidden = 100, size.minibatch = 10)
#RBM com classificação
train <- t(Fashion$trainX)
TrainY <- t(Fashion$trainY)
View(Fashion)
train <- t(Fashion$trainX)
dim(Fashion)
View(Fashion)
View(Fashion)
#Diminui o tamanho para melhorar processamento
Fashion$trainX <- Fashion$trainX[,1:5000]
Fashion$trainY <- Fashion$trainY[,1:5000]
Fashion$trainY <- Fashion$trainY[1:5000]
library(dplyr)
library(RBM)
data(Fashion)
image(matrix(Fashion$trainX[,102], nrow = 28))
Fashion$trainY[102]
#Diminui o tamanho para melhorar processamento
Fashion$trainX <- Fashion$trainX[,1:5000]
Fashion$trainY <- Fashion$trainY[1:5000]
train <- t(Fashion$trainX)
#RBM
modelRBM <- RBM(x = train, n.iter = 1000, n.hidden = 100, size.minibatch = 10, plot = TRUE)
View(Fashion)
#Diminui o tamanho para melhorar processamento
Fashion$trainX <- Fashion$trainX[,1:1000]
Fashion$trainY <- Fashion$trainY[1:1000]
train <- t(Fashion$trainX)
#RBM
modelRBM <- RBM(x = train, n.iter = 1000, n.hidden = 100, size.minibatch = 10, plot = TRUE)
View(Fashion)
library(dplyr)
library(dplyr)
library(RBM)
data(Fashion)
image(matrix(Fashion$trainX[,102], nrow = 28))
Fashion$trainY[102]
Fashion$trainX <- Fashion$trainX[,1:2000]
Fashion$trainY <- Fashion$trainY[1:2000]
Fashion$testX <- Fashion$testX[,2001:2200]
View(Fashion)
Fashion$testX <- Fashion$testX[,1:200]
Fashion$testY <- Fashion$testY[1:200]
train <- t(Fashion$trainX)
#RBM
modelRBM <- RBM(x = train, n.iter = 1000, n.hidden = 100, size.minibatch = 10, plot = TRUE)
test <- t(Fashion$testX)
ReconstructRBM(test = test[102,], model = modelRBM)
#RBM com classificação
train <- t(Fashion$trainX)
TrainY <- t(Fashion$trainY)
modelClassRBM <- RBM(x = train, y = TrainY, n.iter = 1000, n.hidden = 100, size.minibatch = 10)
test <- t(Fashion$testX)
TestY <- t(Fashion$testY)
p <- PredictRBM(test = test, labels = TestY, model = modelClassRBM)
#DBN
modelDBN <- StackRBM(x = train, layers = 100, n.iter = 1000, size.minibatch = 10)
#DBN
modelDBN <- StackRBM(x = train)
ReconstructDBM(test = test[102,], model = modelDBN)
ReconstructRBM(test = test[102,], model = modelDBN)
#DBN
modelDBN <- StackRBM(x = train, leayers = 10)
#DBN
modelDBN <- StackRBM(x = train, layers = 10)
#DBN
modelDBN <- StackRBM(x = train, layers = c(10,10))
ReconstructRBM(test = test[102,], model = modelDBN)
View(modelDBN)
#DBN
modelDBN <- StackRBM(x = train, layers = c(11))
#DBN
modelDBN <- StackRBM(x = train, layers = 11)
length(modelDBN)
#DBN
modelDBN <- StackRBM(x = train, layers = 2)
#DBN
modelDBN <- StackRBM(x = train, layers = c(2,2))
ReconstructRBM(test = test[102,], model = modelDBN)
View(modelDBN)
#DBN
modelDBN <- StackRBM(x = train, layers = c(1,2))
View(modelDBN)
ReconstructRBM(test = test[102,], model = modelDBN)
length(modelDBN)
modStack <- StackRBM(x = train, layers = c(100, 100, 100), n.iter = 1000, size.minibatch = 10)
train <- MNIST$trainX
modStack <- StackRBM(x = train, layers = c(100, 100, 100), n.iter = 1000, size.minibatch = 10)
ReconstructRBM(test = test[100, ], model = modStack, layers = 3)
test <- MNIST$testX
ReconstructRBM(test = test[100, ], model = modStack, layers = 3)
pacotes <- c("devtools","ggplot2","dplyr")
if(sum(as.numeric(!pacotes %in% installed.packages())) != 0){
instalador <- pacotes[!pacotes %in% installed.packages()]
for(i in 1:length(instalador)) {
install.packages(instalador, dependencies = T)
break()}
sapply(pacotes, require, character = T)
} else {
sapply(pacotes, require, character = T)
}
library("devtools")
RBM <- function (x, y, n.iter = 100, n.hidden = 30, learning.rate = 0.1,
plot = FALSE, size.minibatch = 10, momentum = 0.5, lambda = 0.001) {
# Check whether n.iter is devicable by ten and if so initialize plot.epoch:
if (plot == TRUE) {
if ((n.iter %% 10) == 0) {
# Plot at each n.iter/10
plot.epoch <- n.iter/10
} else {
# Warn user and turn plotting off
print ('Number of iterations was not devicable by ten: plots are turned off')
plot <- FALSE
plot.epoch <- 0
}
} else {
# Set plot.epoch to FALSE
plot.epoch <- FALSE
}
# Some checks
if (!is.matrix(x)) {
print('Data was not in a matrix, converted data to a matrix')
x <- as.matrix(x)
}
if (any(!is.numeric(x))) {
stop('Sorry the data has non-numeric values, the function is terminated')
}
if (n.iter > 10000) {
print("Number of epochs for > 10000, could take a while to fit")
}
if (!missing(y)) {
if (any(!is.numeric(y))) {
stop('Sorry the labels have non-numeric values, the function is terminated')
}
if (any(!is.finite(y))) {
stop('Sorry this function cannot handle NAs or non-finite label values')
}
if (length(y) != nrow(x)) {
stop('Labels and data should be equal for supervised RBM: try training an unsupervised RBM')
}
}
if (any(!is.finite(x))) {
stop('Sorry this function cannot handle NAs or non-finite data')
}
if (size.minibatch > 100) {
print('Sorry the size of the minibatch is too long: resetting to 10')
size.minibatch <- 10
}
if (size.minibatch > 20) {
print('Large minibatch size, could take a long time to fit model')
}
if (min(x) < 0 | max(x) > 1) {
stop('Sorry the data is out of bounds, should be between 0 and 1')
}
if( length(dim(x)) < 2 ) {
stop("Dimensions of the data were not right, should be of shape n.features * n.samples")
}
if(ncol(x) > nrow(x)) {
print('Less data than features, this will probably result in a bad model fit')
}
# Initialize the weights, n.features * n.hidden with values from gaussian distribution
weights <- matrix(rnorm(ncol(x) * n.hidden, 0, .01), nrow = ncol(x), ncol = n.hidden)
# Initialize the momentum_speed matrix
momentum_speed_x <- matrix(0, nrow = ncol(x) + 1, ncol = n.hidden + 1)
# Add bias to weights
weights <- cbind(0, weights)
weights <- rbind(0, weights)
# Add 1 for the bias to x
x <- cbind(1, x)
# Initialize the labels, weights and bias for the labels if supervised = TRUE
if (!missing(y)) {
# Get all the unique labels in y
labels <- unique(y)
# Get the indexes of each unique label in y
idx <- vector('list', length = length(labels))
# Save indexes
for (i in 1:length(labels)) {
idx[[i]]<- which(y == labels[i])
}
# Make binarized vectors of the labels
y <- LabelBinarizer(y)
# Add one term for the bias
y <- cbind(1, y)
# Create the y weights matrix
y.weights <- matrix(rnorm(length(labels) * n.hidden, 0, 01), nrow = length(labels), ncol = n.hidden)
# Add momentum speed matrix
momentum_speed_y <- matrix(0, nrow = length(labels) + 1, ncol = n.hidden + 1)
# add bias to weights
y.weights <- cbind(0, y.weights)
y.weights <- rbind(0, y.weights)
}
# PLot the untrained weights
if(plot == TRUE){
# Set plotting margins
par(mfrow = c(3,10), mar = c(3,1,1,1))
plot.weights <- weights[-1, -1]
if (n.hidden > 30) {
# Warn user that only a sample of the hidden nodes will plotted
print('n.hidden > 30, only plotting a sample of the invisible nodes')
# Take sample
samp.plot <- sample(1:n.hidden, 30)
# Remove weights for plotting
for(i in samp.plot) {
# Plot weights
image(matrix(plot.weights[, i], nrow = sqrt(ncol(x)-1)), col=grey.colors(255))
title(main = paste0('Hidden node ', i), font.main = 4)
# Initialize counter for the plotting:
plot.counter <- 0
}
} else {
for(i in 1:n.hidden) {
# Plot weights
image(matrix(plot.weights[, i], nrow = sqrt(ncol(x)-1)), col=grey.colors(255))
title(main = paste0('Hidden node ', i), font.main = 4)
# Initialize counter for the plotting:
plot.counter <- 0
}
}
}
plot.counter <- 0
# Start contrastive divergence, k = 1
for (i in 1:n.iter){
if (missing(y)) {
# Sample minibatch from x, unsupervised
samp <- sample(1:nrow(x), size.minibatch, replace = TRUE)
} else {
# Pick balanced labels
samp <- rep(0,size.minibatch)
p <- 1
for (i in 1 : size.minibatch){
samp[p]<- sample(idx[[p]], 1)
p <- p + 1
if (p == length(labels) +1) {
# Reset counter
p <- 1
}
}
}
plot.counter <- plot.counter + 1
# At iteration set visible layer to random sample of train:
V0 <- x[samp, ,drop = FALSE]
if (missing(y)) {
# Calculate gradients
grads <- CD(V0, weights)
} else {
# Calculate gradients
grads <- CD(V0, weights, y[samp,,drop = FALSE], y.weights)
}
# Update the momentum speed
momentum_speed_x <- momentum * momentum_speed_x + ((grads$grad.weights - (lambda * weights))/ size.minibatch)
# Update weights and bias
weights <- weights + (learning.rate * momentum_speed_x)
if (!missing(y)) {
# Update momentum speed
momentum_speed_y <- momentum * momentum_speed_y + ((grads$grad.y.weights - (lambda * y.weights))/ size.minibatch)
# Update weights and bias
y.weights <- y.weights + (learning.rate * momentum_speed_y)
}
# Plot learning of hidden nodes at every plot.epoch:
if(plot.counter == plot.epoch & plot == TRUE) {
# Create margins
par(mfrow = c(3,10), mar = c(3,1,1,1))
# Remove bias for plottingun
plot.weights <- weights[-1, -1]
if (n.hidden > 30) {
for(i in samp.plot) {
image(matrix(plot.weights[, i], nrow = sqrt(ncol(x)-1)), col=grey.colors(255))
title(main = paste0('Hidden node ', i), font.main = 4)
}
} else {
for(i in 1:n.hidden) {
image(matrix(plot.weights[, i], nrow = sqrt(ncol(x)-1)), col=grey.colors(255))
title(main = paste0('Hidden node ', i), font.main = 4)
}
}
# Reset the plot counter:
plot.counter <- 0
}
}
# Return list with the matrices of trained weights and bias terms
if (!missing(y)) {
return(list('trained.weights' = weights,'trained.y.weights' = y.weights))
} else {
return(list('trained.weights' = weights))
}
}
library("devtools")
install_github("TimoMatzen/RBM")
#install_github("TimoMatzen/RBM")
library("RBM")
#Vamos dar uma olhada na imagem
# visualize the digits
data(MNIST)
force(MNIST)
View(MNIST)
MNIST[["trainY"]]
?image
image(matrix(MNIST$trainX[100, ], nrow = 28))
MNIST$trainY[100]
image(matrix(MNIST$trainX[100, ], nrow = 28))
image(matrix(MNIST$trainX[102, ], nrow = 28))
MNIST$trainY[102]
image(matrix(MNIST$trainX[102, ], nrow = 28))
image(matrix(MNIST$trainX[102, ], ncol = 28))
image(matrix(MNIST$trainX[102, ], nrow = 28))
image(matrix(MNIST$trainX[2, ], nrow = 28))
MNIST$trainY[2]
image(matrix(MNIST$trainX[6, ], nrow = 28))
image(matrix(MNIST$trainX[2, ], nrow = 28))
set.seed(0)
train <- MNIST$trainX
?RBM
modelRBM <- RBM(x = train, n.iter = 1000, n.hidden = 100, size.minibatch = 10, plot = TRUE)
test <- MNIST$testX
test[100,]
testY <- MNIST$testY
testY[100,]
testY[100]
test[100,]
?ReconstructRBM
ReconstructRBM(test = test[100, ], model = modelRBM)
ReconstructRBM(test = test[100, ], model = modelRBM)
testY[100]
ReconstructRBM(test = test[100, ], model = modelRBM)
testY[1]
testY[0]
testY[3]
testY[5]
ReconstructRBM(test = test[8, ], model = modelRBM)
ReconstructRBM(test = test[5, ], model = modelRBM)
library(devtools)
library(RBM)
#Vamos dar uma olhada na imagem
# visualize the digits
data(MNIST)
image(matrix(MNIST$trainX[102, ], nrow = 28))
set.seed(0)
train <- MNIST$trainX
TrainY <- MNIST$trainY
modelClassRBM <- RBM(x = train, y = TrainY, n.iter = 1000, n.hidden = 100, size.minibatch = 10)
View(modelClassRBM)
set.seed(0)
test <- MNIST$testX
TestY <- MNIST$testY
?PredictRBM
p <- PredictRBM(test = test, labels = TestY, model = modelClassRBM)
p
library(devtools)
library(RBM)
set.seed(0)
train <- MNIST$trainX
test <- MNIST$testX
?StackRBM
modStack <- StackRBM(x = train, layers = c(100, 100, 100), n.iter = 1000, size.minibatch = 10)
ReconstructRBM(test = test[100, ], model = modStack, layers = 3)
library("devtools")
#Mostrar pacote no GITHUB
#https://github.com/TimoMatzen/RBM
#install_github("TimoMatzen/RBM")
library("RBM")
#Vamos dar uma olhada na imagem
# visualize the digits
data(MNIST)
image(matrix(MNIST$trainX[2, ], nrow = 28))
MNIST$trainY[2]
set.seed(0)
train <- MNIST$trainX
modelRBM <- RBM(x = train, n.iter = 1000, n.hidden = 100, size.minibatch = 10, plot = TRUE)
set.seed(0)
test <- MNIST$testX
testY <- MNIST$testY
test[100,]
testY[5]
ReconstructRBM(test = test[5, ], model = modelRBM)
library(devtools)
library(RBM)
set.seed(0)
train <- MNIST$trainX
test <- MNIST$testX
modStack <- StackRBM(x = train, layers = c(100, 100, 100), n.iter = 1000, size.minibatch = 10)
ReconstructRBM(test = test[5, ], model = modStack, layers = 3)
library(dplyr)
library(RBM)
set.seed(0)
data(Fashion)
View(Fashion)
Fashion[["trainY"]]
image(matrix(Fashion$trainX[,102], nrow = 28))
Fashion$trainY[102]
Fashion$trainX <- Fashion$trainX[,1:2000]
Fashion$trainY <- Fashion$trainY[1:2000]
Fashion$testX <- Fashion$testX[,1:200]
Fashion$testY <- Fashion$testY[1:200]
train <- t(Fashion$trainX)
#RBM
modelRBM <- RBM(x = train, n.iter = 1000, n.hidden = 100, size.minibatch = 10, plot = TRUE)
test <- t(Fashion$testX)
ReconstructRBM(test = test[102,], model = modelRBM)
